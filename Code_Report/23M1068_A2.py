# -*- coding: utf-8 -*-
"""IML_Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/180FpXO2tTMEp9Y8g8T4IrsmepqxzzEJi

**I have discussed with KARANAM DHANVI with roll no: 23M1190 while doing the assignment**

While doing the assignment, the numpy and pandas library has been used by me,  and they made the things simple where some of the things were done from scratch level in the first assignment. Though i haven't utilised fully the library but i have learnt how to deal with the data sets, processing of the data, how to find correlation among the variables, calclation of MSE, accuracy and so on.

Mainly the assignment MLforSmartMonkeys given by professor has helped a lot for doing the assignment. The things which explained while discussing that weren't clear and it was like an alien at the time of the lecture but while doing the assignment and when I was going through the thing prof has given then i really understood the purpose of doing them.  

**Question 1**
In the question 1a, the dataset is downloaded from the website.
In 1b), the preprocessing of data and how the features are distributed were checked in the similar way prof has explained in the example.
In 1c) The hyperparameter tuning is done by gridsearch by splitting the data into tarining and testing  two sets in 80:20 ratio. It could have been done by splitting the data set into 3 parts namely training, validation and testing, and using the validation for hyper parameter tuning and testing on the test data set. But I have chosen to use Gridsearch from scikit-learn library, with 5 fold cross validation which automatically does the hyper parameter tuning by searching for the optimal combination of hyper paramters for a given model. The models which i have chosen to do are i. random forest and support vector machine with regression model though the data is **discrete**. I haven't done the neural network modelling but I have gone through it how it is to be done. The hyperameters used in SVM are regularisation and kernel width in SVM; max depth and no.of trees in random forest. The MSE i got  in case of SVM with grid search is more than that of Random forest. The comparison of the models with MSE is shown as the observation after the cell.

1d) In this, the relative importance of each feature in a predictive modelling task. From this we can understand the features which have significant importance on target variable so that we can eliminate the less important feature.

1e) In this, the model which is trained on oned data set is used for testing the other data set and checked the performance of it.

**Question 2:**
In the second question the data set is different than the data set used in question 1 and in this we have many nulls which was not in the case of question1. So, inaddition to what we have done in question 1 we have done imputation to deal with the missing variables. And remaining things are similar to question1. But in this question I have used classification model of SVM and Random forest in the question2. The accuracy is used as a parameter for comparing the models.
2d)The recursive feature elimination has been used. By removing the unnecessary features we can eliminate the issue of overfitting.

**Question3**
I have gone through the code which is copied from the website and it has resnet18 convNet is used as feature extractor which is a type of deep neural network. The model is having data set of ants and bees, the code which i run will find whether the selected images of training set in the example it is found for 4 images which can be extended to any no.of images.
In 3b) The function generates the features of the image which is given as input to the function. The ouput produces the dataset dimensions as NX512.
In 3c) Along with the features, labels are also taken as ouput of the training dataset, then using the labels and features, the trained data set is trained with SVM and random forest models and tested them on the testing data set and found the accuracy of prediction.

**Question4**
The SVM model for redwine data set is deployed on a local web server using the streamlit app. The features of the dataset are set as sliders so that based on the proportion of feature the mean square varies.
The question4 has been attached as a seperate file since i have made using spyder. and First 3 questions with the google colab.

The results and observation of each cell is entered afer the cell wherever it is required.
"""

#Importing the libraries required
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pandas.plotting import scatter_matrix #For plotting scatter plot
from sklearn import preprocessing # For data preprocessing
from sklearn.model_selection import train_test_split #For splitting training and testing data

"""# 1.a) Downloading the wine quality datasets from https://archive.ics.uci.edu/ml/datasets/Wine+Quality"""

#The links which are used for the writing the code for Q1 are
#https://chat.openai.com/c/d7ee86a4-61e0-4bd6-92c7-89b6046f2136
#And mainly from the code provided by Amit sethi sir which is MLforSmartMonkeys.ipynb

#Downloading the wine quality datasets from https://archive.ics.uci.edu/ml/datasets/Wine+Quality
dataset_redwine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep = ';')#reading the redwine csv file into data_readwine from the csv files uploaded in the folder
dataset_whitewine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', sep = ';')#reading the whitewine csv file into data_whitewine from the csv files uploaded in the folder

"""# 1. b). Explore, visualize, and pre-process the data as appropriate."""

#Splitting the data set into training and validation
#Creating a split such as 80-20

data_redwine = dataset_redwine.drop(columns=['quality'])#seperating the features and target
y_red = dataset_redwine['quality']

#Quality is the required ouput, so seperating the columns as features and ouput
data_whitewine = dataset_whitewine.drop(columns=['quality'])#seperating the features and target
y_white = dataset_whitewine['quality']

#splitting given data set to 20% testing,80%training data for red and white wine
data_redwine_train, data_redwine_test, y_red_train, y_red_test = train_test_split(data_redwine, y_red, test_size = 0.2, random_state=42)

data_whitewine_train, data_whitewine_test, y_white_train, y_white_test = train_test_split(data_whitewine, y_white, test_size = 0.2, random_state=42)

"""#Now we will check
(i) The data type of every column
(ii) Missing values
(iii) Histograms of all the columns to check the good spread
"""

#Visualisation of datasets Display header and a few rows of redwine and white wine data set

display(data_redwine_train)
display(data_redwine_train.info())

display(data_whitewine_train)
display(data_whitewine_train.info())
print(data_redwine_train.dtypes)
print(data_whitewine_train.dtypes)

"""Observations: All data types are int or float, which are only numerical values"""

#To find the data type, null values, no.of unique values in the given data set
print('----------------------------------------------------------')
print('The diversity in the features of Red wine is as follows:')
print('----------------------------------------------------------')
for col in data_redwine_train.columns.values: #For every column
  list_vals = pd.unique(data_redwine_train[col]) #Creates list of unique values
  print(col + ' is of type' + str(data_redwine_train[col].dtypes) + ', has ' + str(len(list_vals))+ ' unique values, and ' + str(np.sum(pd.isnull(data_redwine_train[col])))+ ' null entries')
  if(len(list_vals)<10): #If column has less than 10 unique values print the values
    list_str = ''
    for i in range(0, len(list_vals)):
      list_str = list_str + str(list_vals[i]) + ','
    print(' These are: '+list_str[0:len(list_str)-2])

print('\n----------------------------------------------------------')
print('The diversity in the features of White wine is as follows:')
print('----------------------------------------------------------')
for col in data_whitewine_train.columns.values: #For every column
  list_vals = pd.unique(data_whitewine_train[col]) #Creates list of unique values
  print(col + ' is of type' + str(data_whitewine_train[col].dtypes) + ', has ' + str(len(list_vals))+ ' unique values, and ' + str(np.sum(pd.isnull(data_whitewine_train[col])))+ ' null entries')
  if(len(list_vals)<10): #If column has less than 10 unique values print the values
    list_str = ''
    for i in range(0, len(list_vals)):
      list_str = list_str + str(list_vals[i]) + ','
    print(' These are: '+list_str[0:len(list_str)-2])

"""Observations:

1. None of the columns have any null entries.
2. Number of unique values are more in many of the columns in both the data tests. So they can be treated as continuous variables.

"""

#Plot histograms for each column of both the data sets
print('---------------------------------------------')
print('Histogram for each column of Red wine data set')
print('---------------------------------------------')
for col in data_redwine_train.columns.values:
  if (data_redwine_train[col].dtypes == 'int64') | (data_redwine_train[col].dtypes == 'float64'):
    plt.hist(data_redwine_train[col])
    plt.xlabel(col)
    plt.show()

print('---------------------------------------------')
print('Histogram for each column of White wine data set')
print('---------------------------------------------')
for col in data_whitewine_train.columns.values:
  if (data_whitewine_train[col].dtypes == 'int64') | (data_whitewine_train[col].dtypes == 'float64'):
    plt.hist(data_whitewine_train[col])
    plt.xlabel(col)
    plt.show()

"""Observation:

From the histogram all the variables are spread well so we can't drop any variable.
"""

#Plotting pair-wise scatter matrices
#Plots every column with all other columns for red wine

scatter_matrix(data_redwine_train, alpha=0.2, figsize=(26, 26), diagonal='kde') # kde is kernel density estimation

#Plotting pair-wise scatter matrices
#Plots every column with all other columns for white wine
scatter_matrix(data_whitewine_train, alpha=0.2, figsize=(26, 26), diagonal='kde') # kde is kernel density estimation

"""The variable pairs whose scatter looks like straight lines or thin curves are correlated. We can eliminate some of them

Variables whose scatter show multiple vertical or horizontal lines are heavily quantised.

Plotting the correlation matrix to visualise the data in a better way. Perfect negative correlation is also a perfect correlation. So we will plot absolute value.
"""

#Correlation matrix plot for red wine
corr_mat_red = data_redwine_train.corr(method = 'spearman')#Spearman used for assesing non-linear relationship among the variable

sns.heatmap(abs(corr_mat_red), annot= False)# Shows the abs value
plt.title('Redwine correlation matrix')
plt.show()

#Correlation matrix plot for white wine
corr_mat_white = data_whitewine_train.corr(method = 'spearman')#Spearman used for assesing non-linear relationship among the variable

sns.heatmap(abs(corr_mat_white), annot= False)# Shows the abs value
plt.title('Whitewine correlation matrix')
plt.show()

#As both red and white wine data set has same type of features, we are printing the rows header of corr matrix
row_head_red = corr_mat_red.index.tolist()

print(f'The total features in the data sets with size {len(row_head_red)} are:\n',row_head_red )

#Makes the values to zero whose correlation is < 0.8 For easy identification for redwine
#For redwine
mod_corr_mat_red = corr_mat_red.copy()

mod_corr_mat_red[abs(mod_corr_mat_red) < 0.8] = 0
corr_mat_red_array = mod_corr_mat_red.to_numpy()
indices = np.where(abs(corr_mat_red_array)>0.8)

rows_red = indices[0]
cols_red = indices[1]
row_head_red = mod_corr_mat_red.index.tolist()
col_head_red = mod_corr_mat_red.index.tolist()
pairs_red = [(row_head_red[row], col_head_red[col]) for row, col in zip(rows_red, cols_red)]
#Remove the duplicate pairs
unique_pairs_red = set()

for pair in pairs_red:
  unique_pairs_red.add(tuple(sorted(pair)))
for pair in unique_pairs_red:
  header_value = pair[0]
  for header in pair:
    mod_corr_mat_red.rename(columns = {header: header_value}, index = {header: header_value}, inplace = True)
row_head_red = mod_corr_mat_red.index.tolist()
col_head_red = mod_corr_mat_red.columns.tolist()
print(row_head_red)

row_headers_unique_red = list(set(row_head_red))

redwine_col_to_retain = row_headers_unique_red
print(f'The features  to be retained in red wine with size {len(redwine_col_to_retain)} are:\n',redwine_col_to_retain )

#Makes the values to zero whose correlation is < 0.8 For easy identification for white wine
#For redwine
mod_corr_mat_white = corr_mat_white.copy()
mod_corr_mat_white[abs(mod_corr_mat_white) < 0.8] = 0

corr_mat_white_array = mod_corr_mat_white.to_numpy()
indices = np.where(abs(corr_mat_white_array)>0.8)
rows_white = indices[0]
cols_white = indices[1]
row_head_white = mod_corr_mat_white.index.tolist()
col_head_white = mod_corr_mat_white.index.tolist()
pairs_white = [(row_head_white[row], col_head_white[col]) for row, col in zip(rows_white, cols_white)]



#Remove the duplicate pairs
unique_pairs_white = set()

for pair in pairs_white:
  unique_pairs_white.add(tuple(sorted(pair)))
for pair in unique_pairs_white:
  header_value = pair[0]
  for header in pair:
    mod_corr_mat_white.rename(columns = {header: header_value}, index = {header: header_value}, inplace = True)
row_head_white = mod_corr_mat_white.index.tolist()
col_head_white = mod_corr_mat_white.columns.tolist()

row_headers_unique_white = list(set(row_head_white))
whitewine_col_to_retain = row_headers_unique_white
print(f'The features  to be retained in white wine with size {len(whitewine_col_to_retain)} are:\n',whitewine_col_to_retain )

"""Observation:

The above code checks the corr coeff values inside the matrix and wherever corr coeff >0.8 between two features, one of them will be retained and one will be removed. The columns which will be retained in both red and white wine is printed. At the time when it is run,  white wine data has a feature which is having corr coeff > 0.8, and it is dropped due to high correlation, for red wine all have corr coeff <0.8.

**Preparing data**\
i) Eliminate useless columns, but in red wine there is no redundancy, no unvarying data, no missing entries. So we can't remove any variables in red wine. In white wine density and alcohol are correlated with 0.82 corr coeff. so we remove it.

ii) Converting discrete variables into one-hot-bit but no such thing in the given data

iii) Normalise the ranges to not to have disproportionate sway.
"""

#Normalising the data by shifting and scaling so that columns are zero mean and unit variance


scaler_red = preprocessing.StandardScaler()#Normalises the data for red is named as  scale_coeff_red

scaler_white = preprocessing.StandardScaler()#Normalises the data for white is named as scale_coeff_white

scaler_red.fit(data_redwine_train[redwine_col_to_retain])#It computes mean and standard deviation for redwine training data set

scaler_white.fit(data_whitewine_train[whitewine_col_to_retain])#It computes mean and standard deviation for whitewine training data set
# col_names_red = data_redwine_train[redwine_col_to_retain].columns
# col_names_white = data_whitewine_train[whitewine_col_to_retain].columns
train_X_red = pd.DataFrame(scaler_red.transform(data_redwine_train[redwine_col_to_retain]))#Uses mean and std deviation
train_Y_red = pd.DataFrame(y_red_train)

print("Mean of red data after normalisation:\n",train_X_red.mean())#Gives the average value of normalised tarianing data
print("Standard deviation of red data after normalisation:\n",train_X_red.std())

train_X_white = pd.DataFrame(scaler_white.transform(data_whitewine_train[whitewine_col_to_retain]))#Uses mean and std deviation
train_Y_white = pd.DataFrame(y_white_train)


print("Mean of white data after normalisation:\n",train_X_white.mean())#Gives the average value of normalised tarianing data
print("Standard deviation of white data after normalisation:\n",train_X_white.std())#Prints the standard deviation of white wine data

corrMat_red  = train_X_red.corr(method = 'spearman')#Checking correlation again for redwine
corrMat_white = train_X_white.corr(method = 'spearman')#Checking correlation again for redwine

#Plots the new correlation matrix for redwine data
sns.heatmap(abs(corrMat_red), annot= False)# Shows the abs value
plt.title('Redwine new correlation matrix')
plt.show()

#Plots the new correlation matrix for white wine
sns.heatmap(abs(corrMat_white), annot= False)# Shows the abs value
plt.title('Whitewine new correlation matrix')
plt.show()

"""Observations:

Standard deviation is 1, mean is nearly zero after the normalisation
"""

#Normalise test data using the same scaler coefficients which was fitted on training

#Testing on redwine
test_X_red = pd.DataFrame(scaler_red.transform(data_redwine_test[redwine_col_to_retain]),columns=redwine_col_to_retain )
test_Y_red = pd.DataFrame(y_red_test)
display(test_X_red)#To display the redwine test data used with validation set of white wine
print(test_X_red.mean())

test_X_white = pd.DataFrame(scaler_white.transform(data_whitewine_test[whitewine_col_to_retain]),columns=whitewine_col_to_retain)
test_Y_white = pd.DataFrame(y_white_test)
display(test_X_white)#To display the redwine test data used with validation set of white wine
print(test_X_white.mean())

"""# c) Train, validate varying at least one hyperparameter, and test at least two types of models: [2]
i. Random forest
ii. Support vector regression with RBF kernel
iii. Neural network with single hidden layer (output layer should have linear activation)

**Using five-fold cross-validation to find the reasonable hyper-parameter settings.**

Instead of splitting the entire data into 3 subsets for training, validation and test it is split only to two training(80%) and testing(20%). It is used gridsearch which performs hyper parameter tuning, by systematiclaly searching the optimal combination of hyperpparameters that are set before the training process. So, the same grid search is used for the 3 models in question (c)

#The links used for  doing the question 3 are
#https://scikit-learn.org/stable/modules/grid_search.html
#https://in.mathworks.com/help/stats/understanding-support-vector-machine-regression.html
#https://stackoverflow.com/questions/20095187/regression-trees-or-random-forest-regressor-with-categorical-inputs
#https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb
#https://chat.openai.com/c/501f7459-f29f-46f1-9e7f-c474a6522a84
"""

#Let's use automated grid search over a range of hyperparameters
from sklearn.model_selection import GridSearchCV #GridsearchCV is used for hyperparameter tuning, evaluates each combination of hyperparameters using crossvalidation
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import mean_squared_error
from sklearn import svm

print('--------------------------------')
print('Training SVR using GridSearchCV')#SVC is a type of SVM for classification tasks
#SVR is used for regression tasks
print('--------------------------------')

# #Training, Validation using Support vector regression with RBF kernel
# #Let the hyperparameter be the regularisation parameter C: [0.1,0.5,1,2,10]. higher values of C leads to smaller margin and overfitting; Lower values of C will lead to larger margin and underfitting
#and kernel width 'gamma': [0.1,0.01,0.001]


##***************************************#
##Modeling the red wine data using SVR##
##**************************************#
scoring = 'neg_mean_squared_error'
svr_model = svm.SVR(kernel= 'rbf') #one vs rest is taken as multi class parameter
hp = {'C': [0.1,0.2,1,5,10] , 'gamma': [0.1,0.01,0.001]}#, 'epsilon': [0.01,0.1,0.2]
grid_search_svr_red = GridSearchCV(estimator = svr_model, param_grid = hp, cv =5, scoring = scoring)
grid_search_svr_red.fit(train_X_red, train_Y_red.values.ravel())#Fitting the model

#Finding the best parameters for red wine data
best_params_red = grid_search_svr_red.best_params_
best_score_red = grid_search_svr_red.best_score_
best_svr_model_red = grid_search_svr_red.best_estimator_

print('Best parameters for red wine:')
print(best_params_red)
y_pred_red = best_svr_model_red.predict(test_X_red)#Predicting the output for the test input
print('The Mean square error for red model:\n', mean_squared_error(test_Y_red,y_pred_red))

##***************************************#
##Modeling the white wine data using SVR##
##**************************************#

#Using grid search for the data which is splitted as training
grid_search_svr_white = GridSearchCV(estimator = svr_model, param_grid = hp, cv =5, scoring = scoring)
grid_search_svr_white.fit(train_X_white, train_Y_white.values.ravel())#Fitting the model

#Finding the best parameters for white wine data
best_params_white = grid_search_svr_white.best_params_
best_score_white = grid_search_svr_white.best_score_
best_svr_model_white = grid_search_svr_white.best_estimator_

print('Best parameters for white wine:')
print(best_params_white)

y_pred_white = best_svr_model_white.predict(test_X_white)#Predicting the output for the test input
print('The Mean square error for white model:\n', mean_squared_error(test_Y_white,y_pred_white))

#Random forest with hyper haparameters as number of trees and max depth of trees
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score

##***************************************#
##Modeling the red wine data using RF##
##**************************************#
scoring = 'neg_mean_squared_error'
rf_model = RandomForestRegressor()
hp = {'max_depth': [2,5,10,20],'n_estimators': [10,20,30,50,100]}

#Using grid search for the data which is splitted as training
grid_search_rf_red = GridSearchCV(estimator = rf_model, param_grid = hp, cv =5, scoring = scoring)
grid_search_rf_red.fit(train_X_red, train_Y_red.values.ravel())#Fitting the model

#Finding the best parameters for white wine data
best_params_red = grid_search_rf_red.best_params_
best_score_red = grid_search_rf_red.best_score_
best_rf_model_red = grid_search_rf_red.best_estimator_

print('Best parameters for red wine:')
print(best_params_red)

y_pred_red = best_rf_model_red .predict(test_X_red)#Predicting the output for the test input
print('The mean square error for red wine data:\n', mean_squared_error(test_Y_red,y_pred_red))


##***************************************#
##Modeling the white wine data using RF##
##**************************************#
#Using grid search for the data which is splitted as training
grid_search_rf_white = GridSearchCV(estimator = rf_model, param_grid = hp, cv =5, scoring = scoring)
grid_search_rf_white.fit(train_X_white, train_Y_white.values.ravel())#Fitting the model

#Finding the best parameters for white wine data
best_params_white = grid_search_rf_white.best_params_
best_score_white = grid_search_rf_white.best_score_
best_rf_model_white = grid_search_rf_white.best_estimator_

print('Best parameters for white wine:')
print(best_params_white)

y_pred_white = best_rf_model_white.predict(test_X_white)#Predicting the output for the test input
print('The mean square error for white wine data:\n', mean_squared_error(test_Y_white,y_pred_white ))

"""It is given in the question that output layer should have linear activation, so regression model of neural networks is done. Mean square error is taken as metric since it is regression model."""

#Neural network with hyper haparameters as number of neurons in hidden layer and max depth of trees
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPRegressor

nn_regressor = MLPRegressor(hidden_layer_sizes=(100,), max_iter= 1000, activation='relu', solver = 'adam', random_state = 42)
#The hyper parameters are considered to be alpha and learning rate
hp = {'alpha':[0.001,0.01,0.1,0.5], 'learning_rate_init':[0.001,0.01,0.1]}

##***************************************#
##Modeling the red wine data using NN##
##**************************************#
grid_search_nn_red = GridSearchCV(estimator = nn_regressor, param_grid = hp, cv =5, scoring = scoring)
grid_search_nn_red.fit(train_X_red, train_Y_red.values.ravel())#Fitting the model
#Finding the best parameters for white wine data
best_params_red = grid_search_nn_red.best_params_
best_score_red = grid_search_nn_red.best_score_
best_nn_model_red = grid_search_nn_red.best_estimator_
y_pred_red = best_nn_model_red.predict(test_X_red)#Predicting the output for the test input
print('Best parameters for white wine with NN model:')
print(best_params_red)
print('The mean square error for red wine data with NN model:\n', mean_squared_error(test_Y_red,y_pred_red ))


##***************************************#
##Modeling the white wine data using NN##
##**************************************#
grid_search_nn_white = GridSearchCV(estimator = nn_regressor, param_grid = hp, cv =5, scoring = scoring)
grid_search_nn_white.fit(train_X_white, train_Y_white.values.ravel())#Fitting the model
#Finding the best parameters for white wine data
best_params_white = grid_search_nn_white.best_params_
best_score_white = grid_search_nn_white.best_score_
best_nn_model_white = grid_search_nn_white.best_estimator_
y_pred_white = best_nn_model_white.predict(test_X_white)#Predicting the output for the test input
print('Best parameters for white wine with NN model:')
print(best_params_white)
print('The mean square error for white wine data with NN model:\n', mean_squared_error(test_Y_white,y_pred_white ))

"""Observation : The comparision of Mean square error in the 3 models is given below
--------------------------------
Data set| Model |MSE
--------|-------|------------------
Red   | SVR|  0.351
Red   | RF|  0.314
Red   | NN|  0.349
White | SVR  | 0.472
White | RF  | 0.353
White | NN  | 0.487

# d. Search the net about how to determine the importance of each variable, and find the importance in the final models tried. Comment on whether the same variables are important for different models.

https://machinelearningmastery.com/calculate-feature-importance-with-python/ \\
https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c
\\ http://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/#example-1-feature-importance-for-classifiers

Feature importance can, therefore, be determined by comparing the size of these coefficients to each other. By looking at the SVM coefficients it is, therefore, possible to identify the main features used in classification and get rid of the not important ones (which hold less variance)
"""

# Commented out IPython magic to ensure Python compatibility.
#Feature importance with SVR model
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
from sklearn.exceptions import ConvergenceWarning

warnings.filterwarnings("ignore", category = UserWarning)
perm_imp_svr_red = permutation_importance(best_svr_model_red, train_X_red, train_Y_red)
fe_imp_svr_red = perm_imp_svr_red.importances_mean
ftr_names = np.array(data_redwine.columns)
plt.figure(figsize=(10,6))
sorted_idx = fe_imp_svr_red.argsort()
plt.barh(ftr_names[sorted_idx],fe_imp_svr_red[sorted_idx])
plt.xlabel("Permutation Importance")
plt.ylabel("feature")
plt.title('Permutation Importance for SVR(Red wine)')
plt.show()

perm_imp_svr_white = permutation_importance(best_svr_model_white, train_X_white, train_Y_white)
fe_imp_svr_white = perm_imp_svr_white.importances_mean
ftr_names = np.array(data_whitewine.columns)
plt.figure(figsize=(10,6))
sorted_idx = fe_imp_svr_white.argsort()
plt.barh(ftr_names[sorted_idx],fe_imp_svr_red[sorted_idx])
plt.xlabel("Permutation Importance")
plt.ylabel("feature")
plt.title('Permutation Importance for SVR(White wine)')
plt.show()

#Prints the features which are important in white wine and red wine models
print(fe_imp_svr_white )
print(fe_imp_svr_red)

#Importance of each variable for red wine data on RF model
fe_imp_rf_red = best_rf_model_red.feature_importances_
ftr_names = np.array(data_redwine.columns)
#plotting the feature importances
plt.figure(figsize=(10,6))
sorted_idx = fe_imp_rf_red.argsort()
plt.barh(ftr_names[sorted_idx],fe_imp_rf_red[sorted_idx])#Feature importance in it's sorted order to find whether the same feature is important for all models
plt.xlabel("Feature Importance")
plt.ylabel("feature")
plt.title('Feature Importance for RF(Red wine)')
plt.show()

#Importance of each variable for white wine data on RF model
fe_imp_rf_white = best_rf_model_white.feature_importances_
ftr_names = np.array(data_whitewine.columns)
#plotting the feature importances
plt.figure(figsize=(10,6))
sorted_idx = fe_imp_rf_white.argsort()##Feature importance in it's sorted order to find whether the same feature is important for all models
plt.barh(ftr_names[sorted_idx],fe_imp_rf_white[sorted_idx])
plt.xlabel("Feature Importance")
plt.ylabel("feature")
plt.title('Feature Importance for RF(White wine)')
plt.show()

"""From the above models, the importance of features is  same in different models. It is not varying from one to other.

# e. Test the model for red with data from white and vice versa, and comment on whether the model for red wines is applicable to white wines and versa or not.
"""

print(redwine_col_to_retain)

#Predicted output for white data tested on red data model trained with SVR
y_white_svr_red = best_svr_model_red.predict(data_whitewine_test)
#The mean square in the above case
print("Mean squared error of best SVR model for White wine data tested on RED DATA MODEL:\n"
           , mean_squared_error(y_white_svr_red,test_Y_white))

#Predicted output for white data tested on red data model trained with Rf
y_white_rf_red = best_rf_model_red.predict(data_whitewine_test)
#The mean square in the above case
print("\nMean squared error of best RF model for White wine data tested on RED DATA MODEL:\n"
           , mean_squared_error(y_white_rf_red,test_Y_white))

#Predicted output for white data tested on red data model trained with NN
y_white_nn_red = best_nn_model_red.predict(data_whitewine_test)
#The mean square in the above case
print("\nMean squared error of best NN model for White wine data tested on RED DATA MODEL:\n"
           , mean_squared_error(y_white_nn_red,test_Y_white))

#Predicted output for red data tested on white data model trained with SVR
y_red_svr_white = best_svr_model_white.predict(test_X_red[whitewine_col_to_retain])
#The mean square in the above case
print("\nMean squared error of best SVR model for red wine data tested on WHITE WINE MODEL:\n"
            ,mean_squared_error(y_red_svr_white,test_Y_red))

#Predicted output for red data tested on white data model trained with RF
y_red_rf_white = best_rf_model_white.predict(test_X_red[whitewine_col_to_retain])
#The mean square in the above case
print("\nMean squared error of best RF model for red wine data tested on WHITE WINE MODEL:\n"
            ,mean_squared_error(y_red_rf_white,test_Y_red))

#Predicted output for red data tested on white data model trained with NN
y_red_nn_white = best_nn_model_white.predict(test_X_red[whitewine_col_to_retain])
#The mean square in the above case
print("\nMean squared error of best NN model for red wine data tested on WHITE WINE MODEL:\n"
            ,mean_squared_error(y_red_nn_white,test_Y_red))

"""Observation : The comparision of Mean square error in the 3 models is given below
--------------------------------
Trained Data set| Test Data set | Model |MSE
--------|--------|----------|------------------           
Red   | White |SVR|  1.00
Red   | White| RF|  1.01
Red   | White | NN|  5426
White | Red| SVR  | 0.644
White | Red | RF  | 0.564
White | Red | NN  | 0.504


--------------------------------
Data set| Model |MSE
--------|-------|------------------
Red   | SVR|  0.314
Red   | RF|  0.308
Red   | NN|  0.349
White | SVR  | 0.353
White | RF  | 0.357
White | NN  | 0.487

**From the above the Mean square error is more when one data is tested on other data model. So if we use like this there will be more error in the predicted output.**

## . Classification:
# a. Download the data to predict Down syndrome in mice from https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression. The prediction problem is to either predict the genotype (binary) using the gene expression variables from DYRK1A_N to CaNA_N

The links used in this question are
https://chat.openai.com/c/2e99751b-a79e-4de0-944e-829ebc63fc5b
https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb
The same links of Question1 are also used for this question too as it is also same except that it is done as a classification problem.
"""

#Downloading the data to predict Down syndrome in mice from https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression.

data_mice = pd.read_excel("https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls")#reading the down syndrome xls data of mice from the URL mentioned above
data_mice_copy = data_mice.copy()
numeric_cols = data_mice.select_dtypes(include=['float64', 'int64']).columns#Selects the columns with numeric columns in the given data set, in our data set we have only float64 and int64
print(data_mice.columns)

print('-------------------------------------------------------------------------------------------')
print('The diversity in the features of down syndrome of mice data is as follows after imputation:')
print('-------------------------------------------------------------------------------------------')
for col in data_mice.columns.values: #For every column
  list_vals = pd.unique(data_mice[col]) #Creates list of unique values
  print(col + ' is of type' + str(data_mice[col].dtypes) + ', has ' + str(len(list_vals))+ ' unique values, and ' + str(np.sum(pd.isnull(data_mice[col])))+ ' null entries')
  if(len(list_vals)<10): #If column has less than 10 unique values print the values
    list_str = ''
    for i in range(0, len(list_vals)):
      list_str = list_str + str(list_vals[i]) + ','
    print(' These are: '+list_str[0:len(list_str)-2])

"""observation:
All data types are float64 for the features, int 64 for Genotype. but some features have more nulls. so to eliminate the missing values imputation is done
classification parameter is considered as class.

There are several ways to deal with missing values in a data set, like SimpleImputer and IterativeImputer. In multivariate feature imputation, at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. The process is repeated for each feature. Here we are using Iterative imputer on the given data set with 100 iterations

https://machinelearningmastery.com/iterative-imputation-for-missing-values-in-machine-learning/
https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html
"""

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imputer = IterativeImputer(max_iter = 10, random_state= 42)#Calling the iterator function for 10 iterations
imputed_data = imputer.fit_transform(data_mice.loc[:,'DYRK1A_N' : 'CaNA_N'])#Imputes the feature matrix

imputed_df = pd.DataFrame(imputed_data)

#Features in the given data (variables)
ftrs_mice = imputed_df
print(ftrs_mice.columns)
data_mice['class'] = data_mice['class'].replace({'c-CS-m':0,'c-SC-m':1,'c-CS-s':2,'c-SC-s':3,'t-CS-m':4,'t-SC-m':5,'t-CS-s':6,'t-SC-s':7})
tgt = data_mice['class']#Prediction problem is to predict the class

#Finding the number of missing values of each variable
missing_vals = imputed_df.isnull().sum()
print("The no.of null entries in the data is :\n",np.count_nonzero(missing_vals))


#Splitting the data set into training and validation
#Creating a split such as 80-20

data_mice_train, data_mice_valid , y_train, y_valid  = train_test_split(ftrs_mice, tgt, train_size = 0.8)

"""Observation:
After imputation there are no null values in the data set \\

Observation:
There are many null entries in some of the variables. We will impute the variables to eliminate null entries.
"""

#Plot histograms for each column of both the data sets
print('---------------------------------------------')
print('Histogram for each column of Mice data set')
print('---------------------------------------------')
for col in data_mice_train.columns.values:
  if (data_mice_train[col].dtypes == 'int64') | (data_mice_train[col].dtypes == 'float64'):
    plt.hist(data_mice_train[col])
    plt.xlabel(col)
    plt.show()

#Correlation matrix plot for red wine
corr_mat_mice = data_mice_train.corr(method = 'spearman')#Spearman used for assesing non-linear relationship among the variable
fig, ax = plt.subplots(figsize=(20,16))
sns.heatmap(abs(corr_mat_mice), annot= False)# Shows the abs value
plt.title('Redwine correlation matrix')
plt.show()

#Makes the values to zero whose correlation is < 0.9 and also the diagonal entries For easy identification
#For redwine
mod_corr_mat_mice = corr_mat_mice.copy()#Modified corr mat for easy visualisation of corr coeff where it is > or < 0.9
corr_mat_mice_array = mod_corr_mat_mice.to_numpy()
indices = np.where(abs(corr_mat_mice_array)>0.9)
rows = indices[0]
cols = indices[1]
row_headers = corr_mat_mice.index.tolist()
col_headers = corr_mat_mice.columns.tolist()
print("The total no.of features in the given data set are:\n",len(row_headers))
print("\nThe total features in the given data set are:\n",row_headers)
pairs = [(row_headers[row], col_headers[col]) for row, col in zip(rows, cols)]

#Remove the duplicate pairs
unique_pairs = set()

for pair in pairs:
  unique_pairs.add(tuple(sorted(pair)))
for pair in unique_pairs:
  header_value = pair[0]
  for header in pair:
    corr_mat_mice.rename(columns = {header: header_value}, index = {header: header_value}, inplace = True)
row_headers = corr_mat_mice.index.tolist()
col_headers = corr_mat_mice.columns.tolist()


row_headers_unique = list(set(row_headers))

print("\nNo.of features with correlation coefficent < 0.9 are:\n",len(row_headers_unique))
cols_to_retain = row_headers_unique #The columns which are to be retained are stored in this array
print("\nThe columns which are to be retained:\n",cols_to_retain)

"""The code in the above cell determines the features whose correlation coefficent >0.9 between two features and retains one of them.

Observation:
so out of 77 features 7 features are removed that means available features=70
"""

# Normalize data (shift and scale so that columns are zero mean and unit variance)
from sklearn import preprocessing

scaler = preprocessing.StandardScaler() # For data normalization

scaler.fit(data_mice_train[cols_to_retain]) # Compute mean and std
train_X = pd.DataFrame(scaler.transform(data_mice_train[cols_to_retain])) # Use mean and std
train_Y = pd.DataFrame(y_train)


print(train_X.mean())#Prints the mean of the training data which will be zero after normalisation

corrMat = train_X.corr(method='spearman') # Check correlation again
fig, ax = plt.subplots(figsize=(12,10))
sns.heatmap(abs(corrMat), annot=False)
plt.show()

# Normalize test data using the same scaler that was fitted on training.

test_X = pd.DataFrame(scaler.transform(data_mice_valid[cols_to_retain]))
test_Y = pd.DataFrame(y_valid)
display(test_Y)
print(test_X.mean())

"""# c. Train, validate varying at least one hyperparameter, and test at least two types of models: [2]
i. Random forest
ii. Support vector classification using RBF kernel
iii. Neural network with single hidden layer (output layer should be have softmax activation)

The confusion Matrix gives a comparison between actual and predicted values. The confusion matrix is a N x N matrix, where N is the number of classes or outputs. The confusion Matrix allows us to measure Recall and Precision, which, along with Accuracy and the AUC-ROC curve, are the metrics used to measure the performance of ML models. The F1 score is the weighted average of precision and recall. For unbalanced data set F1 score is more useful than accuracy. Therefore for balanced class, we will use accuracy
"""

#Support vector classification using RBF Kernel
#Let's use automated grid search over a range of hyperparameters

from sklearn.model_selection import GridSearchCV #GridsearchCV is used for hyperparameter tuning, evaluates each combination of hyperparameters using crossvalidation
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score#To check how accuarate is our predicted data with true data
from sklearn.metrics import make_scorer, f1_score

print('-------------------------------')
print('Training SVC using GridSearchCV')#SVC is a type of SVM for classification tasks
#SVR is used for regression tasks
scoring = 'accuracy'#Since balanced


# #Training, Validation using Support vector machine with RBF kernel for classification
# #Let the hyperparameter be the regularisation parameter C: [0.1,0.5,1,2,10]. higher values of C leads to smaller margin and overfitting; Lower values of C will lead to larger margin and underfitting

from sklearn import svm
svc = svm.SVC(kernel= 'rbf')#Given in the question to use RBF kernel
hp = {'C': [0.1,0.2,1,5,10] , 'gamma': [0.1,0.01,0.001]}#, 'gamma': ['scale', 'auto'], 'epsilon': [0.01,0.1,0.2]}

train_Y = np.ravel(train_Y)
clf_svc = GridSearchCV(estimator = svc, param_grid = hp, cv =5, scoring = scoring)#5- fold cross validation is used
clf_svc.fit(train_X,train_Y)

best_params_svc = clf_svc.best_params_#Best hyper parameters
print(best_params_svc)
best_score_svc = clf_svc.best_score_#Best score
print('Best ' + scoring + ":" + str(best_score_svc))
best_model_svc = clf_svc.best_estimator_#best fit model


print('CLASSIFICATION REPORT')
y_pred_svc = best_model_svc.predict(np.array(test_X))#The output is predicted and the accuracy is printed
print(classification_report(test_Y, y_pred_svc))

"""The accuracy with support vector classification is achieved as 99.88%. So, the feature eliminaton is not done for the SVC."""

#Classification with Random forest with hyper parameters max_depth and no.of trees
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()
hp =  {'max_depth': [2,5,10,20], 'n_estimators': [10,30,100]}
clf_rf = GridSearchCV(estimator = rfc, param_grid = hp, cv =5, scoring = scoring)#5- fold cross validation is used
clf_rf.fit(train_X,train_Y)

#Finds out the best model and parameters
best_params_rf = clf_rf.best_params_
print(best_params_rf)
best_score_rf = clf_rf.best_score_
print('Best ' + scoring + ":" + str(best_score_rf))
best_model_rf = clf_rf.best_estimator_

print('CLASSIFICATION REPORT')
y_pred_rf = best_model_rf.predict(np.array(test_X))
print(classification_report(test_Y, y_pred_rf))#The output is predicted and the accuracy is printed

"""Observation: The classification accuracy is found to be 98% with Random forest model, with best max_depth as 20 and no.of trees = 100.
The error is more compared to SVM in RF.

# d. See if removing some features systematically will improve your models using recursive feature elimination https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html
"""

#https://scikitlearn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html).
#Code has been inspired from the below mentioned website
# https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py
#Importing the libraries
#RFE for random forest
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold

#rfe for random forest
rfecv_rf  = RFECV(estimator =best_model_rf, step =1, cv=5, scoring = 'accuracy',min_features_to_select= 1 )
rfecv_rf.fit(train_X,train_Y)
print(f"Optimal number of features: {rfecv_rf.n_features_}")#optimal No.of features will be calculated and printed

sel_ftrs_rf = train_X.columns[rfecv_rf.support_]#Out of the total features, considers only selected features
rank = rfecv_rf.ranking_#Assigns rank to all the selected features based on its priority

print('Rank of the selected features:\n')
for f, rank in zip(sel_ftrs_rf, rank):
  print(f'{f}: Rank {rank}')

n_scores = len(rfecv_rf.cv_results_["mean_test_score"])
plt.figure(figsize=(10,6))
plt.xlabel('# of features selected')
plt.ylabel('Mean test Accuracy')
plt.title('Recursive feature elimination for RF model')
plt.errorbar(range(1, n_scores + 1),rfecv_rf.cv_results_["mean_test_score"],yerr=rfecv_rf.cv_results_["std_test_score"],)
plt.grid()
plt.show()

"""As the model doesn't have the correlated features the accu racy is constant over the number of features. The optimal model of RFE can lie from the range 20-70 depending on the cross validation technique. Test accuracy remains constant from no.of fatures >20

# 3. Practice using pre-trained neural networks to extract domain-specific features for new tasks.

The links used in the question 3 are
https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html
https://chat.openai.com/c/2e99751b-a79e-4de0-944e-829ebc63fc5b
 and the additional links if any are pasted in the respective sub sections.
"""

#Importing the data from google drive
from google.colab import drive
drive.mount('/content/drive')
folder_path = '/content/drive/MyDrive/hymenoptera_data'

#The code for 3a is directly pasted from the website given in the question and it is run
#Importing the required libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
from PIL import Image
from tempfile import TemporaryDirectory

cudnn.benchmark = True
plt.ion()   # interactive mode

#The code is taken directly from the website https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html and run
# Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_dir = folder_path
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}#Takes the images from the path specified

dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val']}#takes 4 among the data set from train and val folders
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}

class_names = image_datasets['train'].classes #two classes in train folder they are ants and bees
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#Visualize a few images
def imshow(inp, title=None):
    """Display image for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated


# Get a batch of training data
inputs, classes = next(iter(dataloaders['train'])) #Taking features from training
# print(classes)
# Make a grid from batch
out = torchvision.utils.make_grid(inputs)
# out_class = torchvision.utils.make_grid(classes)
# print(out_class)
imshow(out, title=[class_names[x] for x in classes])

"""The number of features in the image is printed as the output from the above code when the input is given as a sample image.

observation:
The code is run succesfully.

# b. Write a function that outputs ResNet18 features for a given input image. Extract features for training images (in image_datasets['train']). You should get an Nx512 dimensional array.
"""

#The links used fro this code https://chat.openai.com/c/2e99751b-a79e-4de0-944e-829ebc63fc5b
#https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html

#Function that outputs ResNet18 features for the given input data set
#It extracts the features for traing images from the given data set and outputs NX512 dimensional array
import torch
import torchvision.models as models
import torchvision.transforms as transforms

def extract_resnet_features(image): #Func definition

    resnet = models.resnet18(pretrained=True)#Loads ResNet18 model


    modules = list(resnet.children())[:-1] # Removing the last layer (the fully connected layer)
    resnet = torch.nn.Sequential(*modules)


    resnet.eval() #Sets the model to evaluation mode

    # Applying transformations to the input image
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])])

    image_tensor = transform(image)
    image_tensor = image_tensor.unsqueeze(0)    #Adds  batch dimension to the input image

    # Input image is passed through the model to obtain the feature vector
    with torch.no_grad():
        features = resnet(image_tensor)
    features = features.squeeze()# Remove the batch dimension
    features = torch.flatten(features)#flattens the feature vector
    return features.numpy()    # Returns feature array

#The example for the above written function
import numpy as np
from torchvision import datasets

# Load the training dataset
data_path = folder_path
image_datasets = datasets.ImageFolder(data_path)

# Extract ResNet18 features for all training images
features_list = []
for i in range(len(image_datasets)):
    image, _ = image_datasets[i]
    features = extract_resnet_features(image)
    features_list.append(features)

# Convert the list of feature vectors to a numpy array
features_array = np.vstack(features_list)

#To print the dimension the output which is NX512
print("ResNet18 features for a given input image set:\n",features_array.shape)

"""In the above 3 cells, function to extract the resnet18 features is written in the first cell, Example is provided in the second cell which takes the input as the given folder which contains set of images. And produces the the size of the array as ouptut in the 3rd cell.

# c. Compare RBF kernel SVM (do grid search on kernel width and regularization) and random forest (do grid search on max depth and number of trees). Test the final model on test data and show the results -- accuracy and F1 score.
"""

#https://chat.openai.com/c/d7ee86a4-61e0-4bd6-92c7-89b6046f2136 links used for this section
#The below function is similar to the one in 3a,3b with few modifications
#Importing the required libraries
import torch
from torchvision import models,datasets,transforms
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,f1_score

# Applying transformations to the input image
transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])])
#Specifying the path to the folders and loaded them to dataset


#The function which is similar to the one in 3b, except that the labels are also returned
def extract_resnet_features(model,data):#Func definition
   model.eval()
   ftrs = []#ftrs array initalisation
   labels = [] #label initialisation
   with torch.no_grad():#Data splitting will be done inside
    for inputs,labels_batch in data:
      features_batch = model(inputs)
      ftrs.append(features_batch)
      labels.append(labels_batch)
   ftrs = torch.cat(ftrs, dim=0)
   labels = torch.cat(labels, dim=0)
   return ftrs.numpy(),labels.numpy()

resnet = models.resnet18(pretrained=True)#Loads ResNet18 model
modules = list(resnet.children())[:-1] # Removing the last layer (the fully connected layer)
resnet = torch.nn.Sequential(*modules)

dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,
                                             shuffle=False, num_workers=4)
              for x in ['train', 'val']}#takes 4 among the data set from train and val folders
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}

class_names = image_datasets['train'].classes #two classes in train folder they are ants and bees
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

train_ftrs, train_labels = extract_resnet_features(resnet,dataloaders['train'])#The features will be extracted from this for training data
test_ftrs, test_labels = extract_resnet_features(resnet,dataloaders['val'])#The features will be extracted from this for testing data

#Performing the RF and SVM on the data set of ants and bees
train_ftrs = train_ftrs.reshape(train_ftrs.shape[0],-1)#reshaping the array for dimension match
test_ftrs = test_ftrs.reshape(test_ftrs.shape[0],-1)#reshaping the array for dimension match
svm_hp = {'C': [0.1,0.2,1,5,10] , 'gamma': [0.1,0.01,0.001]}#Defining the hyper parameters for SVM
rf_hp =  {'max_depth': [2,5,10,20], 'n_estimators': [10,30,100]}#Defining the hyper parameters for RF

#Grid search is performed with SVC and RF models for the given hyperparameter for hyper parameter tuning
svm_grid_search = GridSearchCV(SVC(kernel='rbf'), svm_hp, cv = 5)
rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_hp, cv = 5)
#Best fit of rf model and extracting the best model
rf_grid_search.fit(train_ftrs, train_labels)
best_rf_model = rf_grid_search.best_estimator_

#Best fit of SVM model and extracting the best model
svm_grid_search.fit(train_ftrs, train_labels)
best_svm_model = svm_grid_search.best_estimator_

#Testing the models
#SVM model
svm_predict = best_svm_model.predict(test_ftrs)
svm_accuracy = accuracy_score(test_labels, svm_predict)
svm_f1 = f1_score(test_labels, svm_predict, average = 'weighted')
#RF model
rf_predict = best_rf_model.predict(test_ftrs)
rf_accuracy = accuracy_score(test_labels, svm_predict)
rf_f1 = f1_score(test_labels, rf_predict, average = 'weighted')

#Printing the accuracy and f1 score of the two models
print("Accuracy of SVM model is:\n",svm_accuracy)
print("F1 score of SVM model is:\n",svm_f1)
print("Accuracy of SVM model is:\n",rf_accuracy)
print("F1 score of SVM model is:\n",rf_f1)

"""The question 4, is attached in the another file.
And it is watched from the link
https://www.youtube.com/watch?v=5XnHlluw-Eo&t=252s
"""

pip install streamlit

# -*- coding: utf-8 -*-
"""
Created on Tue Mar 19 16:30:06 2024

@author: MALATHI PALADUGU
"""
#For doing the question 4 I have followed the links mentioned below
#https://www.youtube.com/watch?v=5XnHlluw-Eo&t=252s
#https://chat.openai.com/c/03a2b147-2e4b-41eb-8f6c-da4665631231
#https://www.youtube.com/watch?v=WLwjvWq0GWA
#The RF regression model is applied on the red wine data set and set the sliders for different features in the data set.
#Varying the proportion of features will change the mean square error
#The sliders are used to vary the feature value. The error varies accordingly
#Deployed this model on a local web server using streamlit app.


#Importing the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
import streamlit as st

#Downloading the redwine data from the website
dataset_redwine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep = ';')#reading the redwine csv file into data_readwine from the csv files uploaded in the folder
#Quality has been taken as the target vector, so dropping it from the data set to sepearate data and target
data_redwine = dataset_redwine.drop(columns=['quality'])#seperating the features and target
y_red = dataset_redwine['quality']#Target vector

#splitting the data into training and testing with the ratio 80:20
data_redwine_train, data_redwine_test, y_red_train, y_red_test = train_test_split(data_redwine, y_red, test_size = 0.2, random_state=42)

#Finding the correlation coeffcients
corr_mat_red = data_redwine_train.corr(method = 'spearman')#Spearman used for assesing non-linear relationship among the variable
#For redwine
mod_corr_mat_red = corr_mat_red.copy()

#If the corr coeff <0.8 it is made as 0 and found out the features with
#Corr oeff >0.8 using the below code. It removes the correlated features and final features are stored in cols_to_retain
mod_corr_mat_red[abs(mod_corr_mat_red) < 0.8] = 0
corr_mat_red_array = mod_corr_mat_red.to_numpy()
indices = np.where(abs(corr_mat_red_array)>0.8)

rows_red = indices[0]
cols_red = indices[1]
row_head_red = corr_mat_red.index.tolist()
col_head_red = corr_mat_red.index.tolist()
pairs_red = [(row_head_red[row], col_head_red[col]) for row, col in zip(rows_red, cols_red)]
#Remove the duplicate pairs
unique_pairs_red = set()

for pair in pairs_red:
  unique_pairs_red.add(tuple(sorted(pair)))
for pair in unique_pairs_red:
  header_value = pair[0]
  for header in pair:
    corr_mat_red.rename(columns = {header: header_value}, index = {header: header_value}, inplace = True)
row_head_red = corr_mat_red.index.tolist()
col_head_red = corr_mat_red.columns.tolist()

row_headers_unique_red = list(set(row_head_red))
#redwine_col_to_retain = row_headers_unique_red.columns.tolist()
redwine_col_to_retain = col_head_red

#Preprocessing using standard scaler(normalising) the data
scaler_red = preprocessing.StandardScaler()#Normalises the data for red is named as  scale_coeff_red


scaler_red.fit(data_redwine_train[redwine_col_to_retain])#It computes mean and standard deviation for redwine training data set

#Training data of input and target after normalisation
train_X_red = pd.DataFrame(scaler_red.transform(data_redwine_train[redwine_col_to_retain]), columns = redwine_col_to_retain)#Uses mean and std deviation
train_Y_red = pd.DataFrame(y_red_train)

#Testing on redwine data
test_X_red = pd.DataFrame(scaler_red.transform(data_redwine_test[redwine_col_to_retain]),columns = redwine_col_to_retain)
test_Y_red = pd.DataFrame(y_red_test)

##***************************************#
##Modeling the red wine data using RF##
##**************************************#
#Even though the target variable is integer
#Regression has been used in this question as it is mentioned to do so
scoring = 'neg_mean_squared_error'#scoring parameter is taken as mean square error
rf_model = RandomForestRegressor()
hp = {'max_depth': [2,5,10,20],'n_estimators': [10,20,30,50,100]}

#Using grid search for the data which is splitted as training
grid_search_rf_red = GridSearchCV(estimator = rf_model, param_grid = hp, cv =5, scoring = scoring)
grid_search_rf_red.fit(train_X_red, train_Y_red.values.ravel())#Fitting the model



#Finding the best parameters for white wine data
best_params_red = grid_search_rf_red.best_params_#best hyperparameters can be found
best_score_red = grid_search_rf_red.best_score_
best_rf_model_red = grid_search_rf_red.best_estimator_#Finds the best model
rf_model_red = RandomForestRegressor(max_depth = best_params_red['max_depth'], n_estimators = best_params_red['n_estimators'])  # calling the model and passing optimum hyperparameter
rf_model_red.fit(train_X_red, train_Y_red.values.ravel())
y_pred_red = best_rf_model_red .predict(data_redwine_test)#Predicting the output for the test input



st.title('Wine Quality Predictor')

#Giving slider for each feature
fixed_acidity = st.slider('fixed acidity', float(data_redwine['fixed acidity'].min()), float(data_redwine['fixed acidity'].max()), float(data_redwine['fixed acidity'].mean()))
volatile_acidity = st.slider('volatile acidity', float(data_redwine['volatile acidity'].min()), float(data_redwine['volatile acidity'].max()), float(data_redwine['volatile acidity'].mean()))
citric_acid = st.slider('citric acid', float(data_redwine['citric acid'].min()), float(data_redwine['citric acid'].max()), float(data_redwine['citric acid'].mean()))
residual_sugar = st.slider('residual sugar', float(data_redwine['residual sugar'].min()), float(data_redwine['residual sugar'].max()), float(data_redwine['residual sugar'].mean()))
chlorides = st.slider('chlorides', float(data_redwine['chlorides'].min()), float(data_redwine['chlorides'].max()), float(data_redwine['chlorides'].mean()))
free_sulfur_dioxide = st.slider('free sulfur dioxide', float(data_redwine['free sulfur dioxide'].min()), float(data_redwine['free sulfur dioxide'].max()), float(data_redwine['free sulfur dioxide'].mean()))
total_sulfur_dioxide = st.slider('total sulfur dioxide', float(data_redwine['total sulfur dioxide'].min()), float(data_redwine['total sulfur dioxide'].max()), float(data_redwine['total sulfur dioxide'].mean()))
density = st.slider('density', float(data_redwine['density'].min()), float(data_redwine['density'].max()), float(data_redwine['density'].mean()))
pH = st.slider('pH', float(data_redwine['pH'].min()), float(data_redwine['pH'].max()), float(data_redwine['pH'].mean()))
sulphates = st.slider('sulphates', float(data_redwine['sulphates'].min()), float(data_redwine['sulphates'].max()), float(data_redwine['sulphates'].mean()))
alcohol = st.slider('alcohol', float(data_redwine['alcohol'].min()), float(data_redwine['alcohol'].max()), float(data_redwine['alcohol'].mean()))

#All the features are saved into it
Features = pd.DataFrame({'fixed acidity': [fixed_acidity], 'volatile acidity': [volatile_acidity], 'citric acid': [citric_acid],
                         'residual sugar': [residual_sugar], 'chlorides': [chlorides], 'free sulfur dioxide': [free_sulfur_dioxide],
                         'total sulfur dioxide': [total_sulfur_dioxide], 'density': [density], 'pH': [pH], 'sulphates': [sulphates],
                         'alcohol': [alcohol]})
prediction = rf_model_red.predict(Features)#Value of the feature is taken
mse = mean_squared_error(test_Y_red,y_pred_red )#Mean square error is computed


st.subheader('Predicted Wine Quality')

st.write(prediction)
st.subheader('Mean square error with the given parameters')
st.write(mse)